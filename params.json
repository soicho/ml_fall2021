{"name":"News Topic Classification","tagline":"","body":"## Introduction/Background \r\nHuman is a social animal and there are lots of events going on in society. People carry those social issues through news articles, and since numerous different issues are happening, these news articles have a lot of different categories. Classification of documents in these categories can bring huge advantages to the application. It can analyze users’ interests in news articles and recommend related articles that users might find useful, or it can measure the number of articles on different topics to evaluate the hottest issues that happened in a certain timeline and so on.\r\n\r\nOur team aims to implement document classification models of news topics due to the massive application possibilities it can bring. We use the AG news dataset which is a collection of more than 1 million new articles gathered from more than 2000 news sources in more than 1 year of activity.\r\n\r\n## Problem Definition\r\nThe focus of our project is to accurately classify the topic of the news article. We plan on using both supervised and unsupervised Machine Learning algorithms for classification. We aim to implement document classification models of news topics due to the massive application possibilities they can bring.\r\n\r\n\r\n## Dataset\r\n\r\n[AG's corpus of news articles](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.htmls)\r\n\r\nThe data we are using is called AG news. The dataset contains 120,000 training datasets containing a short header for a news article and a corresponding label for the header. There are four possible classes for the labels, World, Sports, Business, and Sci/Tech. Since the dataset comes with the ground truth labels, we can try to come up with a classification model that will correctly label an article header. We can also try different clustering algorithms to see if we can find any interesting structures within the articles as it’s rather reasonable to believe that articles covering different topics would have a different distribution of words. \r\n\r\nThere are a little over 4.6 million word tokens in the data with roughly 165,000 unique word tokens where the word tokens that appear most frequently are words like common stop words such as “the”, “to”, and “a\". If we use all these word tokens the dimension of our dataset would be 165,000 with only 120,000 datasets which is not a good thing to have. \r\n\r\nLooking at the frequency of words, we can also see that roughly half, 46.5%, of the words only appear once in the entire corpus. These words include names such as “Bazil” and “Tearaway” which only gets used once in the entire dataset because a specific article mentions them. These words are so rare they don’t really contribute to building a meaningful classifier as the chance that they’ll appear in the test dataset is rare. So by having these extremely rare words in our training data, we are in a way overfitting our model that may perform well on the training set but won’t do well on our test dataset. \r\n\r\n\r\n### Data Exploration\r\nWord Cloud or Tag Clouds is a visualization technique for texts natively used for visualizing tags or keywords. In the Word Cloud, the size of each word is based on the frequency of words in each document. So, we can not only explore the frequency of words but also by doing so, we can figure out the thesis of the class. \r\n\r\n\r\n![](./image/midterm/WordCloud.png)\r\n\r\n_<Figure 1> 4 Word Cloud describes classes. The text size is based on the frequency of words._\r\n\r\nThese 4 Word Cloud plots show the result. In the first plot, we can see various countries including the largest word, Iraq. We can conclude that the first label is about world news. \r\nThe second plot is composed of various sports-related words and times such as season, win, and game. Also, we can see various sports-related words like Olympic, World Cup, and Red Sox. Based on this information, we can conclude that the second class is about sports. \r\nThe most notable words in the third plot are US, New York, and oil price. Not only that, the third plot shows various business-related words such as investors, company, and million. We can see that the 3rd class news is about business. The last plot shows various tech companies including Microsoft, IBM, and Google. We can confidently assume that the last class is about Science and Technology. \r\n\r\n### Data Preprocessing\r\n\r\n**Stemming and Lemmatization**\r\n\r\nAnother type of data preprocessing that we need to perform is stemming and lemmatization. A lot of word tokens in our documents convey the same meaning but have varying forms due to grammatical reasons. The word “learn” would be a good example of this. Both the phrases 'learn' and 'learns' have the exact same meaning, however, they are used in different scenarios to be in line with English grammar. Treating them as separate entity leads us to have more dimensions to our data than what our data actually conveys. By lemmatizing them into their basic form of 'learn', we can maintain the meaning behind the word while reducing the dimension of our dataset.\r\n\r\nFor example, there are 121, 15, 64, and 114 occurrences of words “learn”, “learns”, “learning”, “learned” respectively before lemmatizing and there are 407, 0, 20, 0 occurrences after lemmatizing. After applying a similar process to all word tokens in our data, we end up with 78,362 unique words in our data which is less than half of what we started with.\r\n\r\n![](./image/midterm/image3.png)\r\n\r\n_<Figure 2> Word Frequencies_\r\n\r\n**Vectorizing**\r\n\r\nOne of the challenges in classifying documents is that since the dataset we are dealing with is simply a string of words, the machine learning models that we use cannot handle words as they are. So we need a way to represent them that our machine learning models can understand and use. Since our models can only understand numerical data, we need to be able to represent the strings of words as a series of numbers for each instance of data. \r\n\r\nAnother thing to keep in mind is that even though there may be many different ways to represent words as numerical values, it would be very helpful if the numeric representation of the words actually helps us maintain how the occurrence of different words represents different classes of documents in our dataset. With that in mind, there are two possible ways to represent words as numeric vectors that our models can use. \r\n\r\n**Count Vectorizer**\r\n\r\nCount vectorizer is a very simple way to convert a string of words into a vector of numbers that represent the frequency of how certain words occur in our dataset. As the name suggests we would simply be counting the occurrence of each word in our dataset at the document level, and then represent our data as a vector of frequencies. \r\n\r\nTo give a concrete example, we can take the sentences “The quick brown fox jumps over the lazy dog” and “The lazy brown fox jumps over the lazy dog” and represent them as follows \r\n\r\n|   |The|Quick|Brown|Fox|Jumps|Over|The|Lazy|Dog|\r\n|---|---|---|---|---|---|---|---|---|---|\r\n|Sent 1|1|1|1|1|1|1|1|1|1|\r\n|Sent 2|1|0|1|1|1|1|1|2|1|\r\n\r\n_<Table 1> Using this method we can quickly convert strings of words into a vector of numeric values that our models can use as inputs._\r\n\r\nThere are two different types of Count Vectorizer one can use, one being a binary vectorizer that tracks whether a specific word has appeared in a document or not and the other being a regular count vectorizer that gives us the count of how often a specific word appeared in a document. If one believes that the frequency of the words imparts some meaningful representation then a binary count vectorizer should not be used. We tested both methods out empirically to see which one gives a better performance overall instead of picking one from the start. \r\n\r\n**Term Frequency Inverse Document Frequency**\r\n\r\nSimilar to Count Vectorizer, TF-IDF can also convert a string of words into a vector of numeric values that our model can use. As the name suggests, Term Frequency is one of the components of the method which is essentially equal to the Count Vectorizer. However, unlike count vectorizer, TF-IDF helps us figure out words that are important by also accounting for inverse document frequency. \r\n\r\nDocument Frequency simply represents how many of the documents in our dataset contains a specific word\r\n\r\nBecause we are dividing N by the document frequency, we are essentially making it so that terms that appear in every document have less weight in our data representation. When we use Count Vectorizer or Term Frequencies, we are in a lot of ways treating each word equally, however, IDF helps us represent words with different weights that can weigh down the terms if the terms appear very frequently and weigh up the terms if they are relatively rare. \r\n\r\nThis makes sense if one thinks about common stop words such as ‘The’ or ‘a’. These words pretty much appear in almost every English sentence and thus do not help us classify or cluster our documents. So words that show up often in our documents will have lower TF-IDF values whereas words that show up less frequently will have higher TF-IDF values. \r\n\r\nThis is great as TF-IDF helps with turning a string of words into a numeric representation while also giving us a more nuanced way of representing our words and what they actually represent in our data. \r\n\r\n## Methods\r\n\r\n**Models and Cross-Validation**\r\n\r\nNow that we’ve preprocessed our data and we need to have a model pipeline built out. In order to test out different models and vectorization algorithms, it’s important to use cross-validation to test out different hyperparameters and algorithms. \r\nFor the classification models, I decided to use some traditional machine learning models such as Naive Bayes, Logistic Regression, Random Forest, and K-nearest neighbors.\r\n\r\nThe hyperparameters used are relatively straightforward for models such as Naive Bayes or KNN classifier as there are only a handful of hyperparameters to use such as how much additive smoothing such as Laplace smoothing we should try adding to our data to deal with potential sparsity in our data such as words with 0 counts. \r\n\r\nAs for KNN, for every data point that we have the algorithm is essentially trying to find K number of data points that are most similar and using their labels as a class label for the data we are trying to predict, the number of neighbors is the most important hyperparameter that we need to tune. So in order to find the value that gives us the best performance, we tested out 10 different values from 3 to 50. \r\n\r\nFor Logistic Regression, there are a few hyperparameters that we tried out. The first one is the type of regularization penalty that we used. We have an option of either using the L2 norm or the L1 norm which changes the way the model is fit. L1 norm can potentially give us sparse feature representation that drives the weights for some features to be 0 while L2 norm keeps all the features in the model. The elastic net penalty was also tested out that treats our model’s penalty as a convex combination of L1 and L2 penalties. Lastly, different values for the strength of regularization were tested out to find the set of hyperparameters that have the best predictive power. \r\n\r\nFor Random Forest we have two big hyperparameters to tune, one being the number of models we fit and the other being the depth of the model. Random Forest is an ensemble model which means that it fits many models with different initial splits in the hopes that by having a large number of predictors, we can have a more robust predictive performance. The other hyperparameter, maximum depth, controls how many features the model will use. This is an important hyperparameter as Decision Trees are known to completely overfit given a very large feature size. So the rationale behind this hyperparameter is that by limiting the number of splits, we can prevent our model from overfitting. \r\n\r\n\r\n## Results and Discussion \r\n\r\nFor cross-validation, accuracy was used to track model performance. Of the 4 models used, Logistic Regression had the highest accuracy overall on the test dataset with 91.2%. The hyperparameters used were the “L2” norm with a regularization parameter of 3.34. Similar accuracy can be achieved using the “L1” norm as well as the accuracy with the “L1” norm came out to be around 90.67%.\r\n\r\nNaive Bayes had the second-best performance behind Logistic Regression with roughly 90% accuracy using simple Laplace Smoothing by adding 1 to all occurrences of words. \r\n\r\nNext cross-validation was used on KNN with varying numbers of neighbors. The model using 18 neighbors had an accuracy of roughly 90%. This is quite similar to Naive Bayes, but since the KNN model would need to find 18 closest neighbors during the test time, it has a higher computation time compared to other methods during test time. \r\n\r\nLastly, Random Forest had the lowest test accuracy at around 88%. For Random Forest, maximum depths of 5000 had the best performance which was the maximum value for the hyperparameter tested. Essentially the more the feature the model used, the better the performance. \r\n\r\n**SVD Dimensionality Reduction**\r\n\r\nGiven that we have data with very large dimensions, one of the ways we can try to streamline data processing and modeling fitting is using some sort of dimensionality reduction to reduce the size of the dimension of the data. \r\n\r\nSVD was used to reduce the dimension of the data as we had some sparsity in our dataset. SVD is similar to PCA in that it projects the data into a lower-dimensional space while trying to retain the essence of the data. \r\n\r\nIn our pipeline, SVD was added right after vectorization so that we can use cross-validation to see what size of dimension leads to the best performance. \r\n\r\nFor the Logistic Regression model, the dimension of 500 was selected from a range of 2 to 500. This led to the data retaining roughly 31% of the variance in the original data. 500 is quite small compared to the 21414 dimensions the original data had, and what’s even more interesting is that the model was able to get close to 89% accuracy on the test dataset. This is quite impressive given that the full dataset gave us an accuracy of roughly 91%. So by reducing the dimension of the data from 21414 to 500, we were still able to retain most of the performance of the data. This helps quite a lot during test time as the size of the model is undoubtedly smaller making it fit in systems with smaller memory. \r\n\r\nA scatter plot showing the first 2 classes using features 7 and 8 is shown below. As one can see even though there are a lot of points that overlap, there are regions in the plot where the data is clearly separated. We can clearly see the classes clustering rather nicely. This shows that projecting the data onto the lower-dimensional space is quite promising. \r\n\r\n![](./image/midterm/image2.png)\r\n\r\n_<Figure 3> A scatter plot showing the first 2 classes_\r\n\r\nA plot with the same features but with all 4 labels is shown below. Similar to the case with 2 classes, there is a lot of overlap but still, there are clear clusters that different models will be able to exploit to fit a classification model.\r\n\r\n![](./image/midterm/image5.png)\r\n\r\n_<Figure 4> A scatter plot showing 4 labels_\r\n\r\n**CNN-based Approach (Supervised Learning)**\r\n\r\nThere have been several approaches using convolutional neural networks for text classification. One of the most popular approaches is “Character-level Convolutional Networks for Text Classification”. They proposed a convolutional neural network with 6 convolution layers followed by 3 fully connected layers. Max pooling is used to reduce the feature dimensions (the authors of the paper emphasized that they were not able to train deep convolutional networks without pooling layers). Since we are not familiar with implementing neural networks (yet), we have used the public implementation from here. We are planning to understand the implementation in more detail and extend this model in the second half to achieve higher accuracy by changing the model structure or using better hyperparameters. \r\n\r\nTo train the model, we used LR 0.0001 with batch size 32. Around 5-6 epochs were required to reach the peak training/test accuracy, 88.5%. We used checkpoints to understand how the loss and accuracies change over time during the training.\r\n\r\n![](./image/midterm/image4.png)\r\n\r\n_<Figure 5> A line graph explaining how the loss and accuracies change over time during the training._\r\n\r\nA few interesting observations:\r\n* We first trained the model from scratch using a MacBook Pro’16, 2019. It took more than a few hours to finish just a few epochs. We have also tried to use a machine with 2 NVIDIA Titan RTX GPUs to accelerate the training with a data-parallel fashion and observed that the training was able to finish more than a hundred epochs in a few hours.\r\n* We were expecting the model to be overfitted after 100 epochs since the loss/accuracy reached a plateau of around 5 epochs. Interestingly, even with the model trained with 100 epochs, the accuracy was similar to the accuracy of a model trained with 5 epochs, which seems contradictory to our intuition. It might be possible that the learning rate is too high to overfit the model or the problem is easy enough to achieve ~90% accuracy.\r\n* Even though the model was able to achieve pretty high accuracy, it is still not straightforward how the CNN can learn the pattern of “text” data. Compared with other approaches, the most surprising fact is that this CNN model does not require explicit “words”.\r\n* The training was very sensitive to the learning rate. When we use 0.001 instead of 0.0001 as the learning rate, the peak accuracy was only 25%, which means it is not learning anything at all considering that we only have 4 labels. This shows the importance of hyperparameter tuning.\r\n\r\n**K-Means Approach (Unsupervised Learning)**\r\n\r\nOne of the unsupervised learning approaches we could take from this dataset is K-Means. Using tf-idf vectorized dataset, disregarding all the information about labels for each data, we could do the k-means approach.\r\n\r\n![](./image/midterm/image8.png)\r\n\r\n_<Figure 6>The Elbow method for K-means clustering to find the best K._\r\n\r\n### Next Steps\r\n\r\nGiven that we have used accuracy as the main metric to track performance, we can definitely try out some different metrics to see if other models make more sense for us. We might try using precision as that tells us of the ones the model predicted, how many were actually correctly predicted. This could be very important depending on the goal of models and how costly false positives are compared to false negatives. \r\n\r\nAnother metric we’ll look into is the AUC-ROC curve which helps us measure how true positive rate and false positive rate trades off depending on different thresholds for decisions, and this could also be very beneficial depending on the ultimate goal of the model.\r\n\r\n\r\n\r\n\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}